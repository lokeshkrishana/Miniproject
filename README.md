## TITLE 
AI BASED PICTURE TRANSLATION

## About
<!--Detailed Description about the project-->
Language limitations continue to be a significant impediment to successful communication and comprehension in a time of unparalleled global connectivity. Transcending linguistic barriers has significant ramifications for a variety of industries, from tourism and education to healthcare and international trade. The merging of computer vision and natural language processing explores the revolutionary potential of artificial intelligence (AI) in overcoming these obstacles.It focuses particularly on the developing field of AI-based picture translation, a scientific advancement that will fundamentally alter how people engage with visual content in a variety of language contexts. This research aims to clarify the methodology underlying picture translation and throw light on its numerous applications by utilizing cutting-edge methods like Convolutional Neural Networks (CNNs) and sequence-to-sequence models. This study ultimately aims to highlight the enormous promise this subject has, while also addressing the obstacles that lie ahead in this growing world of AI-driven language solutions. This is accomplished through a thorough exploration of case studies and assessment metrics.

## Features
<!--List the features of the project as shown below-->
1.Text Detection and Extraction
2.Language Detection
3.Real-Time Translation
4.Augmented Reality (AR) Overlays
5.Contextual Understanding

## Requirements
<!--List the requirements of the project as shown below-->
* Operating System: Requires a 64-bit OS (Windows 10 or Ubuntu) for compatibility with deep learning frameworks.
* Development Environment: Python 3.6 or later is necessary for coding the sign language detection system.
* Deep Learning Frameworks: TensorFlow for model training, MediaPipe for hand gesture recognition.
* Image Processing Libraries: OpenCV is essential for efficient image processing and real-time hand gesture recognition.
* Version Control: Implementation of Git for collaborative development and effective code management.
* IDE: Use of VSCode as the Integrated Development Environment for coding, debugging, and version control integration.
* Additional Dependencies: Includes scikit-learn, TensorFlow (versions 2.4.1), TensorFlow GPU, OpenCV, and Mediapipe for deep learning tasks.

## System Architecture
<!--Embed the system architecture diagram as shown below-->
![Screenshot 2024-11-13 091457](https://github.com/user-attachments/assets/c91c7fd3-534a-46c8-80d8-b55fdf686805)



## Output

<!--Embed the Output picture at respective places as shown below as shown below-->
#### Output1 - Name of the output
![Screenshot 2024-11-13 091828](https://github.com/user-attachments/assets/e93ba831-6a71-47cd-ac43-2bf915f21bee)



#### Output2 - Name of the output
![Screenshot 2024-11-13 091723](https://github.com/user-attachments/assets/b60b7cee-4425-4a68-8c1d-c9f3d5e3bf3c)



Detection Accuracy: 96.7%
Note: These metrics can be customized based on your actual performance evaluations.


## Results and Impact
<!--Give the results and impact as shown below-->
A wider audience can access information in photographs thanks to AI-based picture translation apps that remove language barriers. Users who may not be fluent in the language can nevertheless easily understand the information.The program makes use of cutting-edge technology, such as deep learning models for machine translation and language identification, as well as image processing and OCR for text detection and extraction.

This project serves as a foundation for future developments in assistive technologies and contributes to creating a more inclusive and accessible digital environment.

## Articles published / References
[1]A. Panayiotou, A. Gardner, S. Williams, E. Zucchi, M. Mascitti-Meuter et al., “Language translation apps in health care settings: Expert opinion,” JMIR mHealth and uHealth, vol. 7, no. 4, pp. e11316, 2019.
[2]C. Tsihouridis, D. Vavougios, M. Batsila and G. Ioannidis, “The optimum equilibrium when using experiments in teaching where virtual and real labs stand in science and engineering teaching practice,” International Journal of Emerging Technologies in Learning (iJET), vol. 14, no. 23, pp. 67– 84, 2019.




